models:
  apertus-70b-prod:
    instances: 1
    model_name: "swissai/Apertus-70B-Instruct"
    model_path: "/capstor/store/cscs/swissai/infra01/swiss-alignment/checkpoints/Apertus-70B-sft-mixture-8e-aligned/"
    model_args: ""
    sub_process: "sp serve ${MODEL_PATH} --served-model-name ${MODEL_NAME} --max-prefill-tokens 8192 --context-length 8192 --tp-size 4 --host 0.0.0.0 --port 8080"
    environment: "/capstor/store/cscs/swissai/a09/xyao/llm_service/sp-dev.toml"
    serving_engine: "sp"
    time_limit: "12:00:00"
    ocf_version: "v0.1.4"
  apertus-8b-prod:
    instances: 1
    model_name: "swissai/Apertus-8B-Instruct"
    model_path: "/capstor/store/cscs/swissai/infra01/swiss-alignment/checkpoints/Apertus-8B-sft-mixture-8e-aligned/"
    model_args: ""
    sub_process: "sp serve ${MODEL_PATH} --served-model-name ${MODEL_NAME} --max-prefill-tokens 8192 --context-length 8192 --tp-size 4 --host 0.0.0.0 --port 8080"
    environment: "/capstor/store/cscs/swissai/a09/xyao/llm_service/sp-dev.toml"
    serving_engine: "sp"
    time_limit: "12:00:00"
    ocf_version: "v0.1.4"
  llama-prod:
    instances: 0
    model_name: "Llama-3.3-70B-Instruct"
    model_path: "meta-llama/Llama-3.3-70B-Instruct"
    model_args: ""
    sub_process: "python3 -m sglang.launch_server --model-path ${MODEL_PATH} --host localhost --max-prefill-tokens 32768 --context-length 32768 --port 8080 --tp-size 4 ${PARSER_ARGS}"
    environment: "/capstor/store/cscs/swissai/a09/xyao/llm_service/sgl.toml"
    serving_engine: "sgl"
    time_limit: "12:00:00"
    ocf_version: "v0.1.4"
  qwen-prod:
    instances: 0
    model_name: "Qwen/Qwen3-32B"
    model_args: "--reasoning-parser qwen3 --tool-call-parser qwen25"
    model_path: "Qwen/Qwen3-32B"
    sub_process: "python3 -m sglang.launch_server --model-path ${MODEL_PATH} --host localhost --max-prefill-tokens 32768 --context-length 32768 --port 8080 --tp-size 4 ${PARSER_ARGS}"
    environment: "/capstor/store/cscs/swissai/a09/xyao/llm_service/sgl.toml"
    serving_engine: "sgl"
    time_limit: "12:00:00"
    ocf_version: "v0.1.4"
  llama-test:
    instances: 0
    model_name: "Llama-3.3-70B-Instruct"
    model_path: "meta-llama/Llama-3.3-70B-Instruct"
    model_args: ""
    sub_process: "python3 -m vllm.entrypoints.openai.api_server --model ${MODEL_PATH} --tensor-parallel-size 4 --host localhost --port 8080 --max-num-batched-tokens 32768 --max-model-len 32768"
    environment: "/capstor/store/cscs/swissai/a09/xyao/llm_service/sgl.toml"
    serving_engine: "vllm"
    time_limit: "12:00:00"
    ocf_version: "v0.1.4"


system_name: "bristen"


client_id: "6ade4584-868f-44b8-bffa-80054fd8bcfa"
client_secret: "env:F7T_CLIENT_SECRET"
token_uri: "https://auth.cscs.ch/auth/realms/firecrest-clients/protocol/openid-connect/token"
firecrest_uri: "https://api.cscs.ch/ml/firecrest/v2"
account: "a-infra01"
bootstrap_addr: "/ip4/148.187.108.172/tcp/43905/p2p/QmcMpnf39qfJcXssHrFFw7nvAioLd4SXKhzBZ4XMcLDoSU"
